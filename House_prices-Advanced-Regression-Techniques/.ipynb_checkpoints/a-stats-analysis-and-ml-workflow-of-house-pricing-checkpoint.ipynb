{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "35a27c2cd534c5bb5b6faaea45ace27ab54d9204"
   },
   "source": [
    "![image](https://www.livetradingnews.com/wp-content/uploads/2017/01/home-sales-701x526.jpg)\n",
    "## <div style=\"text-align: center\" > A Statistical Analysis & Machine Learning Workflow of House-Pricing </div>\n",
    "<div style=\"text-align: center\"> Being a part of Kaggle gives me unlimited access to learn, share and grow as a Data Scientist. In this kernel, I want to solve <font color=\"red\"><b>House Pricing with Advanced Regression Analysis</b></font>, a popular machine learning dataset for <b>beginners</b>. I am going to share how I work with a dataset step by step  <b>from data preparation and data analysis to statistical tests and implementing machine learning models.</b> I will also describe the model results along with many other tips. Let's get started.</div>\n",
    "\n",
    "\n",
    "***\n",
    "<div style=\"text-align:center\"> If there are any recommendations/changes you would like to see in this notebook, please <b>leave a comment</b> at the end of this kernel. Any feedback/constructive criticism would be genuinely appreciated. If you like this notebook or find this notebook helpful, Please feel free to <font color=\"red\"><b>UPVOTE</b></font> and/or leave a comment.\n",
    "    \n",
    "<div> <b>This notebook is always a work in progress. So, please stay tuned for more to come.</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3373aadc93e27b85911747bbef45a35ab6cb9ba4"
   },
   "source": [
    "# Goals\n",
    "This kernel hopes to accomplish many goals, to name a few...\n",
    "*  Learn/review/explain complex data science topics through write-ups. \n",
    "* Do a comprehensive data analysis along with visualizations. \n",
    "* Create models that are well equipped to predict better sale price of the houses. \n",
    "\n",
    "# Introduction\n",
    "This kernel is the \"regression siblings\" of my other [ Classification kernel](https://www.kaggle.com/masumrumi/a-statistical-analysis-ml-workflow-of-titanic). As the name suggests, this kernel goes on a detailed analysis journey of most of the regression algorithms.  In addition to that, this kernel uses many charts and images to make things easier for readers to understand.\n",
    "\n",
    "# 1: Importing Necessary Libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "                                                                        \n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "## Removes warnings. \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bdbd5e13ab7f6027a5d9ec44577c2ae3a56bf1d3"
   },
   "source": [
    "## A Glimpse of the datasets.\n",
    "> **Sample Train Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "9a87653bb3ef9f0663d1a781654a76d2c831bf6c"
   },
   "outputs": [],
   "source": [
    "## Import Trainning data. \n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "580d05cffbe13025476b3ce693ede9595ca6f9c2"
   },
   "source": [
    "> ** Sample Test Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "2fc17f2d38a0cac887ee29f72b9e63eb07924a22"
   },
   "outputs": [],
   "source": [
    "## Import test data.\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1889ec78e513dbb5785b35e8eac62c6ceb61c48d"
   },
   "source": [
    "I am a big fan of visualizing the raw data to get an idea of each variable. However, we will mostly focus on our target variable here and will try to find out the relationship of other features with the target variable. I strongly encourage everyone to check out the data description part to get a better understanding of the following visuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "a74293d4e5d1445bfb3c8d66f6c65c19b1cd5bc2"
   },
   "outputs": [],
   "source": [
    "## Converting some of the numerical variables which is actually categorical. Except year features since those will be too clutered. \n",
    "## the \"OverallCond\" and \"OverallQual\" of the house. \n",
    "train['OverallCond'] = train['OverallCond'].astype(str) \n",
    "train['OverallQual'] = train['OverallQual'].astype(str)\n",
    "\n",
    "## Important years and months that should be categorical variables not numerical. \n",
    "#train['YearBuilt'] = train['YearBuilt'].astype(str)\n",
    "#train['YearRemodAdd'] = train['YearRemodAdd'].astype(str)\n",
    "#train['YrSold'] = train['YrSold'].astype(str)\n",
    "train['MoSold'] = train['MoSold'].astype(str) \n",
    "#train['GarageYrBlt'] = train['GarageYrBlt'].astype(str)\n",
    "\n",
    "## Zoning class are given in numerical; therefore converted to categorical variables. \n",
    "train['MSSubClass'] = train['MSSubClass'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "03649e0c21027b4a3b6121bd290f55f79a68828a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# creating barplots for variable with object type and exploring their relation with \"SalePrice\"\n",
    "for i in range(len(train.columns)):\n",
    "    if train[train.columns[i]].dtype == object:\n",
    "        plt.subplots(figsize = (20,5));\n",
    "        sns.barplot(y = train[\"SalePrice\"], x = train[train.columns[i]]);\n",
    "        plt.title(\"Average Sale Price across {}'s\".format((train.columns[i]), y = 1.05, fontsize = 15));\n",
    "        plt.ylabel(\"SalePrice\");\n",
    "        plt.xlabel(str(train.columns[i]));\n",
    "        plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "986b360c1c27089114b7bba6b05730e77926258f"
   },
   "source": [
    "## Describe the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "9494cf9233d6c6c2ecd87386aef74afc33434f96",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (\"Total rows and columns in train dataset respectively: \" + str(train.shape))\n",
    "print (\"Total rows and columns in test dataset respectively: \" + str(test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3c9d0d9d151aa21deffaa81ccec3349f41207feb"
   },
   "source": [
    "If you want to know more about why we are splitting dataset's into train and test, please check out this [kernel](https://www.kaggle.com/masumrumi/a-statistical-analysis-ml-workflow-of-titanic). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "e368016b1973b5e43953f90723afaf76dc2d89d6"
   },
   "outputs": [],
   "source": [
    "# gives us statistical info about the numerical variables. \n",
    "train.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "52d05e71a4cb0fb14ce6f03ed88ee31e67602262"
   },
   "outputs": [],
   "source": [
    "## Gives us information about the features. \n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "9700c21df8ba309f36c3c5df8dba3145e8127736"
   },
   "outputs": [],
   "source": [
    "## Gives use the count of different types of objects.\n",
    "train.get_dtype_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "240af7020f9799eff5ee6f70dc498d028abc31e0"
   },
   "source": [
    "## Observation\n",
    "* There are multiple types of features. \n",
    "* Some features have missing values. \n",
    "* Most of the features are object( includes string values in the variable).\n",
    "\n",
    "I want to focus our attention on target variable which is **SalePrice.** Let's create a histogram to see if the **SalePrice** feature is Normally distributed. If we want to create any linear model, it is essential that the features are normally distributed. This is one of the assumptions of multiple linear regression. I will explain more on this later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "9a36b87ef98121d0f79e3d7e083e0aa8646ac267",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Importing seaborn, matplotlab and scipy modules. \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy import stats\n",
    "\n",
    "## Creating a customized chart. and giving in figsize and everything. \n",
    "fig = plt.figure(constrained_layout=True, figsize=(15,10))\n",
    "## creating a grid of 3 cols and 3 rows. \n",
    "grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n",
    "#gs = fig3.add_gridspec(3, 3)\n",
    "\n",
    "## Customizing the histogram grid. \n",
    "ax1 = fig.add_subplot(grid[0, :2])\n",
    "## Set the title. \n",
    "ax1.set_title('Histogram')\n",
    "## plot the histogram. \n",
    "sns.distplot(train.SalePrice, norm_hist=True, ax = ax1)\n",
    "\n",
    "# customizing the QQ_plot. \n",
    "ax2 = fig.add_subplot(grid[1, :2])\n",
    "## Set the title. \n",
    "ax2.set_title('QQ_plot')\n",
    "## Plotting the QQ_Plot. \n",
    "stats.probplot(train['SalePrice'], plot = ax2)\n",
    "\n",
    "## Customizing the Box Plot. \n",
    "ax3 = fig.add_subplot(grid[:, 2])\n",
    "## Set title. \n",
    "ax3.set_title('Box Plot')\n",
    "## Plotting the box plot. \n",
    "sns.boxplot(train.SalePrice, orient='v', ax = ax3 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "10bfa43e9abc59f04715503cc96cf3089d9c02f4"
   },
   "source": [
    "These **three** charts above can tell us a lot about our target variable.\n",
    "* Our target variable, **SalePrice** is not normally distributed.\n",
    "* Our target variable is right-skewed. \n",
    "* There are multiple outliers in the variable. \n",
    "\n",
    "\n",
    "**P.S.** \n",
    "* If you want to find out more about how to customize charts, try [this](https://matplotlib.org/tutorials/intermediate/gridspec.html#sphx-glr-tutorials-intermediate-gridspec-py) link. \n",
    "* If you are learning about Q-Q-plots for the first time. checkout [this](https://www.youtube.com/watch?v=smJBsZ4YQZw) video. \n",
    "* You can also check out [this](https://www.youtube.com/watch?v=9IcaQwQkE9I) one if you have some extra time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "4b27877b7c9ca82561cdafceeadbb49352838240"
   },
   "outputs": [],
   "source": [
    "#skewness and kurtosis\n",
    "print(\"Skewness: \" + str(train['SalePrice'].skew()))\n",
    "print(\"Kurtosis: \" + str(train['SalePrice'].kurt()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e7cd72721b3151d27ea8e05db411e75779f2e1ec"
   },
   "source": [
    "It looks like there is quite a bit skewness in the variable. We can fix this by using different types of transformation(more on this later). However, before doing that, I want to find out the relationships between the target variable with other predictor variables. Let's find out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "e1cd31b45aeada2833a48b0393d0c3f87371776a"
   },
   "outputs": [],
   "source": [
    "## Getting the correlation of all the features with target variable. \n",
    "(train.corr()**2)[\"SalePrice\"].sort_values(ascending = False)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "74728f0016b8540efb9a6728b4d45b94f93e1393"
   },
   "source": [
    "These are the predictor variables sorted in a descending order starting with the most correlated one **OverallQual**. Let's put this one in a scatter plot and see how it looks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "46b888ed4c082e1b0330b628a8cdcc0226eb09ed"
   },
   "source": [
    "### SalePrice vs OverallQual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "111a21653dcf08b9f89c64d4cfca7cb80f2805db"
   },
   "outputs": [],
   "source": [
    "## Sizing the plot. \n",
    "plt.subplots(figsize = (20,12))\n",
    "## Plotting target variable with predictor variable(OverallQual)\n",
    "sns.scatterplot(y = train.SalePrice, x = train.OverallQual);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "54f39a39e345b9445f61122446f41ef9b3c37c0a"
   },
   "source": [
    "Clearly, **OverallQual** is a categorical variable and is not the best way to visualize categorical variables. However, it looks like some of the houses are overpriced compared to their overall quality. These could be outliers. Let's check out some more features to determine the outliers, and this time we will do it right and focus on the variables that are numerical variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bc20050a8193f722163f205d086396a37f559289"
   },
   "source": [
    "### SalePrice vs GrLivArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "41972e2b1c1e50dc5a0f4c9888f20a9d6cc95f13"
   },
   "outputs": [],
   "source": [
    "## Sizing the plot. \n",
    "plt.subplots(figsize = (20,12))\n",
    "## Plotting target variable with predictor variable(GrLivArea)\n",
    "sns.scatterplot(y = train.SalePrice, x = train.GrLivArea);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0abb4cd0f96543f9ffd0c682d2d9c46f9893914e"
   },
   "source": [
    "Let's try to keep a mental picture of each one of these charts as we pass through.  I will talk about all of them together. How about the next one?\n",
    "\n",
    "### SalePrice vs GarageArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "838c8c6c0596aef69f1928c6d9fa1d4b2d509dbf"
   },
   "outputs": [],
   "source": [
    "## plot sizing. \n",
    "plt.subplots(figsize = (20,12))\n",
    "## Plotting target variable with predictor variable(GarageArea)\n",
    "sns.scatterplot(y = train.SalePrice, x = train.GarageArea);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e6509a491d9f4d4fa2e448e6fd1fbcba587b6c98"
   },
   "source": [
    "And the next one..?\n",
    "\n",
    "### SalePrice vs TotalBsmtSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "ffd177dba8ef36170082d8483ecb62874dbc5dd0"
   },
   "outputs": [],
   "source": [
    "## plot sizing. \n",
    "plt.subplots(figsize = (20,12))\n",
    "## Plotting target variable with predictor variable(TotalBsmtSF)\n",
    "sns.scatterplot(y = train.SalePrice, x = train.TotalBsmtSF);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d1f723fd23ee61480f296653441fc0116082c4f3"
   },
   "source": [
    "and the next ?\n",
    "### SalePrice vs 1stFlrSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "c97876aed71f6f5b996404d9000b963d51494a21",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Plot sizing.\n",
    "plt.subplots(figsize = (20,12))\n",
    "## Plotting target variable with predictor variable(1stFlrSF)\n",
    "sns.scatterplot(y = train.SalePrice, x = train['1stFlrSF']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "99da17abffb2392c03824e478bc20bf396c944c4"
   },
   "source": [
    "How about one more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "e894b497b8bd664ff93f4c5d7776c3d3c6971965"
   },
   "outputs": [],
   "source": [
    "## Plot sizing. \n",
    "plt.subplots(figsize = (20,12))\n",
    "## Plotting target variable with predictor variable(1stFlrSF)\n",
    "sns.scatterplot(y = train.SalePrice, x = train.MasVnrArea, );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8d3dcd283103a4adbbf1b1f7862af56f726b9a22"
   },
   "source": [
    "Okay, I think we have seen enough. Let's discuss what we have found so far. \n",
    "\n",
    "# Observations\n",
    "* Our target variable shows an unequal level of variance across most predictor(independent) variable values. This is called **Heteroscedasticity**; and is a red flag for the multiple linear regression model.\n",
    "* There are many outliers in the scatter plots above that took my attention. \n",
    "\n",
    "* The two next to the top right edge of **SalePrice vs. GrLivArea** seems to follow a trend, which can be explained by saying that \"As the prices increased so did the area. \n",
    "* However, The two on the bottom right of the same chart do not follow any trends. We will get rid of these two. \n",
    "\n",
    "Let's remember this chart as we may need to discuss it later on. I am saving a copy of this dataset so that any changes, later on, can be compared side by side. I think that is the best way to learn. Let's find some other outliers in other predictor features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "8c050d1c1812cd8cd28dcf0acb40355a2dc8a8bf"
   },
   "outputs": [],
   "source": [
    "## save a copy of this dataset so that any changes later on can be compared side by side.\n",
    "previous_train = train.copy()\n",
    "\n",
    "## Deleting those two values with outliers. \n",
    "train.drop(index = train[(train.GrLivArea > 4000) & (train.SalePrice < 300000)].index.tolist(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "554d4608c15c118ef482695392df44a2f77219e9"
   },
   "source": [
    "As I look through these scatter plots, I realized that it is time to explain the assumptions of Multiple Linear Regression. Let us focus on that.  \n",
    "\n",
    "Before building a multiple linear regression model, we need to check that these assumptions are valid. Below, I have described those assumptions.\n",
    "\n",
    "* **Linearity.** \n",
    "* **Multivariate Normality. **\n",
    "* **Homoscedasticity. **\n",
    "* **Independence of Errors. **\n",
    "* **No or little Multicollinearity. **\n",
    "***\n",
    "We go through each feature and trying to check for the assumptions above. Try this [link](https://www.statisticssolutions.com/assumptions-of-linear-regression/) as it explains everything pretty well. \n",
    "\n",
    "**Linearity: **\n",
    "Linear regression needs the relationship between the independent and dependent variables to be linear.  It is also imperative to check for outliers since linear regression is sensitive to outlier effects.  The linearity assumption can best be tested with scatter plots, the following two examples depict two cases, where no and little linearity is present. The following charts make this topic a bit more clear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "25a4187dc899cf9e492cc172716fbb91d70c7e9e"
   },
   "outputs": [],
   "source": [
    "## Plot sizing. \n",
    "fig, (ax1, ax2) = plt.subplots(figsize = (20,6), ncols=2,sharey=False)\n",
    "## Scatter plotting for SalePrice and GrLivArea. \n",
    "sns.scatterplot( x = train.GrLivArea, y = train.SalePrice,  ax=ax1)\n",
    "## Putting a regression line. \n",
    "sns.regplot(x=train.GrLivArea, y=train.SalePrice, ax=ax1)\n",
    "\n",
    "## Scatter plotting for SalePrice and MasVnrArea. \n",
    "sns.scatterplot(x = train.MasVnrArea,y = train.SalePrice, ax=ax2)\n",
    "## regression line for MasVnrArea and SalePrice. \n",
    "sns.regplot(x=train.MasVnrArea, y=train.SalePrice, ax=ax2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0a7571b3f98d7b0acf4201666a999b42eecf9ec1"
   },
   "source": [
    "Here we are plotting our target variable with two independent variable **GrLivArea** and **MasVnrArea**. It's pretty obvious from the chart that there is a better linear relationship between **SalePrice** and **GrLivArea** than **SalePrice** and **MasVnrArea** \n",
    "***\n",
    "\n",
    "**Multivariate Normality: **\n",
    "The linear regression analysis requires all variables to be multivariate normally distributed.  A histogram or a Q-Q-Plot can check whether the data is normally distributed or not. Normality can be checked with a goodness of fit test, e.g., the Kolmogorov-Smirnov test.  When the data is not normally distributed, a non-linear transformation (e.g., log-transformation, Box-Cox transformation) might fix this issue. We already know that our target variable does not follow a normal distribution. Let's bring back the three charts that show our target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "a37a7cdc9f7979fb47bcad3e2e22292b91f733b3"
   },
   "outputs": [],
   "source": [
    "## Creating a customized chart. and giving in figsize and everything. \n",
    "fig = plt.figure(constrained_layout=True, figsize=(15,10))\n",
    "## creating a grid of 3 cols and 3 rows. \n",
    "grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n",
    "#gs = fig3.add_gridspec(3, 3)\n",
    "\n",
    "## Customizing the histogram grid. \n",
    "ax1 = fig.add_subplot(grid[0, :2])\n",
    "## Set the title. \n",
    "ax1.set_title('Histogram')\n",
    "## plot the histogram. \n",
    "sns.distplot(train.SalePrice, norm_hist=True, ax = ax1)\n",
    "\n",
    "# customizing the QQ_plot. \n",
    "ax2 = fig.add_subplot(grid[1, :2])\n",
    "## Set the title. \n",
    "ax2.set_title('QQ_plot')\n",
    "## Plotting the QQ_Plot. \n",
    "stats.probplot(train['SalePrice'], plot = ax2)\n",
    "\n",
    "## Customizing the Box Plot. \n",
    "ax3 = fig.add_subplot(grid[:, 2])\n",
    "## Set title. \n",
    "ax3.set_title('Box Plot')\n",
    "## Plotting the box plot. \n",
    "sns.boxplot(train.SalePrice, orient='v', ax = ax3 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bc0a89ef21fb39f38e513e16333412491be3d432"
   },
   "source": [
    "Now, let's make sure that the target variable follows a normal distribution. If you want to learn more about probability plot(Q-Q plot), try [this](https://www.youtube.com/watch?v=smJBsZ4YQZw) video. You can also check out [this](https://www.youtube.com/watch?v=9IcaQwQkE9I) one if you have some extra time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "9285248aca6e4c09ef3411f0e41ed727fc3e34cb"
   },
   "outputs": [],
   "source": [
    "## trainsforming target variable using numpy.log1p, \n",
    "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "b33d2d1215052158e4bfef129406f9c84f97b6fe"
   },
   "outputs": [],
   "source": [
    "## Creating a customized chart. and giving in figsize and everything. \n",
    "fig = plt.figure(constrained_layout=True, figsize=(15,10))\n",
    "## creating a grid of 3 cols and 3 rows. \n",
    "grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n",
    "#gs = fig3.add_gridspec(3, 3)\n",
    "\n",
    "## Customizing the histogram grid. \n",
    "ax1 = fig.add_subplot(grid[0, :2])\n",
    "## Set the title. \n",
    "ax1.set_title('Histogram')\n",
    "## plot the histogram. \n",
    "sns.distplot(train.SalePrice, norm_hist=True, ax = ax1)\n",
    "\n",
    "# customizing the QQ_plot. \n",
    "ax2 = fig.add_subplot(grid[1, :2])\n",
    "## Set the title. \n",
    "ax2.set_title('QQ_plot')\n",
    "## Plotting the QQ_Plot. \n",
    "stats.probplot(train['SalePrice'], plot = ax2)\n",
    "\n",
    "## Customizing the Box Plot. \n",
    "ax3 = fig.add_subplot(grid[:, 2])\n",
    "## Set title. \n",
    "ax3.set_title('Box Plot')\n",
    "## Plotting the box plot. \n",
    "sns.boxplot(train.SalePrice, orient='v', ax = ax3 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "73d9569255d4f3ae7bfbc5a510e5b64e6f5ee9c3"
   },
   "source": [
    "Now we see that our target variable looks more normally distributed and all the charts looks more symmetrical. We can use this trick to transform other dependent variables that do not follow a Normal distribution. \n",
    "\n",
    "**Homoscedasticity:** The assumption of homoscedasticity is crucial to linear regression models. Homoscedasticity describes a situation in which the error term or variance or the \"noise\" or random disturbance in the relationship between the independent variables and the dependent variables is same across all values of the independent variables. If the \"noise\" is not same across the values of an independent variable, we call that **Heteroscedasticity**. As you can tell, it is the opposite of **Homoscedasticity.**\n",
    "\n",
    "<p><img src=\"http://www.statisticssolutions.com/wp-content/uploads/2010/01/mlr07.jpg\" style=\"float:left\"><img src=\"http://www.statisticssolutions.com/wp-content/uploads/2010/01/mlr06.jpg\" style=\"float:left ; margin-right :2%; margin-bottom:2%\"></p>\n",
    "<br>\n",
    "\n",
    "These two pictures are good examples of Heteroscedasticity, and if we see this kind of funnel shape in the data, we ought to transform the data using a transformation method like log-transformation or box-cox transformation. Regarding this kernel, a good example of Heteroskedasticity is the first scatter plot (SalePrice vs. GrLivArea). Before we look at it again, remember we have transformed our **SalePrice** be more like a standard Normal curve and that is one way sometimes we can solve the homoscedasticity problem as well. Let's compare how the transformation changed the target variable by placing charts side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "f7c6866f044cd71bef81ffdfa137b06a145c3f8d"
   },
   "outputs": [],
   "source": [
    "## Customizing grid for two plots. \n",
    "fig, (ax1, ax2) = plt.subplots(figsize = (20,6), ncols=2, sharey = False, sharex=False)\n",
    "## doing the first scatter plot. \n",
    "sns.scatterplot(x = previous_train.GrLivArea, y = previous_train.SalePrice, ax = ax1)\n",
    "## doing the scatter plot for GrLivArea and SalePrice. \n",
    "sns.scatterplot(x = train.GrLivArea, y = train.SalePrice, ax = ax2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5bb4f111b56467ca65709fe9eaccf1689cbb5c34"
   },
   "source": [
    "Here, you can see that the pre-transformed chart on the left has heteroscedasticity and the post-transformed chart right has an equal amount of variance across the target variable and therefore follows the assumption of Homoscedasticity.\n",
    "\n",
    "**No or Little multicollinearity:** \n",
    "Multicollinearity is when there is a strong correlation between independent variables. Linear regression or multilinear regression requires independent variables to have little or no similar features. Heatmap is an excellent way to identify whether there is multicollinearity or not. Let's check it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "e448dadb8982495ce5cae131d3adf8b1fa897130"
   },
   "outputs": [],
   "source": [
    "## Plot fig sizing. \n",
    "plt.subplots(figsize = (30,20))\n",
    "## Plotting heatmap. \n",
    "sns.heatmap(train.corr(), cmap=\"BrBG\", annot=True, center = 0, );\n",
    "## Give title. \n",
    "plt.title(\"Heatmap of all the Features\", fontsize = 30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "035886c19d106cdfa5ddaae129f4bdf8b0448fe1"
   },
   "source": [
    "## Observation. \n",
    "As you can see multicollinearity still exists in various features. However, we will keep them for now for the sake of learning and lets models(e.x. Regularization models such as Lasso, Ridge) do the clean up later on. Let's go through some of the correlations that still exists. \n",
    "\n",
    "* There is 0.83 or 83% correlation between **GarageYrBlt** and **YearBuilt**. \n",
    "* 83% correlation between **TotRmsAbvGrd ** and **GrLivArea**. \n",
    "* 89% correlation between **GarageCars** and **GarageArea**. \n",
    "* Similarly many other features such as**BsmtUnfSF**, **FullBath** have good correlation with other independent feature but not so much with the dependent feature.\n",
    "\n",
    "If I were using only multiple linear regression, I would be deleting these features from the dataset in order to fit a better multiple linear regression algorithms. However, we will be using many algorithms as scikit learn modules makes it easy to implement them and get the best possible model. \n",
    "\n",
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "6c41264eb3d91ba0437c9fe191a2c226f50416f0"
   },
   "outputs": [],
   "source": [
    "## saving the train and test id for later. \n",
    "train_id = train['Id']\n",
    "test_id = test['Id']\n",
    "\n",
    "## Dropping the \"Id\" from train and test set. \n",
    "train.drop(columns=['Id'],axis=1, inplace=True)\n",
    "test.drop(columns=['Id'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "6fbfe48ffe405944e8552a315c44f0ca8ac86f6f"
   },
   "outputs": [],
   "source": [
    "## Getting the count of train and test. \n",
    "n_train = train.shape[0]\n",
    "n_test = test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "50ff395824e62a14be6fb1eaae5f15d0c5ba2321"
   },
   "outputs": [],
   "source": [
    "## Saving the target values in \"y_train\". \n",
    "y_train = train.SalePrice.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "7cc426fc9bab60a81e82659869137a24b5e4e67a"
   },
   "outputs": [],
   "source": [
    "## Combining train and test datasets together so that we can do all the work at once. \n",
    "all_data = pd.concat((train, test)).reset_index(drop = True)\n",
    "## Dropping the target variable. \n",
    "all_data.drop(['SalePrice'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "c41693df74927857d4f9a78e81374c84c5be651f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Plot sizing. \n",
    "plt.subplots(figsize = (30,20))\n",
    "## plotting heatmap.  \n",
    "sns.heatmap(all_data.corr(), cmap=\"BrBG\", annot=True, center = 0, );\n",
    "## Set the title. \n",
    "plt.title(\"Heatmap of all the Features\", fontsize = 30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7757cf1014f36cc6062992a49eb008382ad37ae6"
   },
   "source": [
    "When we combined both train and test set, this is how the heatmap looks like. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2925aa32a7b0b6d274c839be4663975b02c9a8d7"
   },
   "source": [
    "## Dealing with Missing Values\n",
    "> **Missing data in train and test data(all_data)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "01cff443aee5682089e59d06fde79db216a41836"
   },
   "outputs": [],
   "source": [
    "## Creating a DataFrame with total missing values and total percentage. \n",
    "temp = pd.DataFrame(all_data.isnull().sum()[all_data.isnull().sum() != 0].sort_values(ascending = False),columns=[\"Total_missing\"])\n",
    "\n",
    "temp[\"Percent\"] = round(temp.Total_missing/len(all_data)*100,2)\n",
    "\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "921c630fb505e81548e2741a1b846cb9843d0229"
   },
   "source": [
    "> **Imputing Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "84f189ef492c206d9132e7b9300c3a16fbd8742c"
   },
   "outputs": [],
   "source": [
    "## Some missing values are intentionally left blank, for example: In the Alley feature \n",
    "## there are blank values meaning that there are no alley's in that specific house. \n",
    "missing_val_col = [\"Alley\", \n",
    "                   \"PoolQC\", \n",
    "                   \"MiscFeature\",\n",
    "                   \"Fence\",\n",
    "                   \"FireplaceQu\",\n",
    "                   \"GarageType\",\n",
    "                   \"GarageFinish\",\n",
    "                   \"GarageQual\",\n",
    "                   \"GarageCond\",\n",
    "                   'BsmtQual',\n",
    "                   'BsmtCond',\n",
    "                   'BsmtExposure',\n",
    "                   'BsmtFinType1',\n",
    "                   'BsmtFinType2',\n",
    "                   'MasVnrType']\n",
    "\n",
    "for i in missing_val_col:\n",
    "    all_data[i] = all_data[i].fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "07e67944fbba52b615156c521a31c7319746d228"
   },
   "outputs": [],
   "source": [
    "## These features are continous variable, we used \"0\" to replace the null values. \n",
    "missing_val_col2 = ['BsmtFinSF1',\n",
    "                    'BsmtFinSF2',\n",
    "                    'BsmtUnfSF',\n",
    "                    'TotalBsmtSF',\n",
    "                    'BsmtFullBath', \n",
    "                    'BsmtHalfBath', \n",
    "                    'GarageYrBlt',\n",
    "                    'GarageArea',\n",
    "                    'GarageCars',\n",
    "                    'MasVnrArea']\n",
    "\n",
    "for i in missing_val_col2:\n",
    "    all_data[i] = all_data[i].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7745e0e7d4c64dfe59976940d054d78c60ceb069"
   },
   "source": [
    "**Creating New feature from other features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "7d5a50f9a832ffdb92af399c0a2769993e484c9c"
   },
   "outputs": [],
   "source": [
    "## Replaced all missing values in LotFrontage by imputing the median value of each neighborhood. \n",
    "all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform( lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "3f0b75a0046595d357a3db871d5c9090312ff335"
   },
   "outputs": [],
   "source": [
    "## For these variables I have used the mode of each variable. \n",
    "missing_val_col3 = ['MSZoning','Utilities','Functional', 'SaleType','KitchenQual','Exterior2nd','Exterior1st','Electrical']\n",
    "\n",
    "for i in missing_val_col3:\n",
    "    all_data[i] = all_data[i].fillna(all_data[i].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "c1f93a6790d66e9d9d5fd21a130b1f6e0d5f921d"
   },
   "outputs": [],
   "source": [
    "## Checking to see if we have any more missing value left. \n",
    "temp = pd.DataFrame(all_data.isnull().sum()[all_data.isnull().sum() != 0].sort_values(ascending = False),columns=[\"Total_missing\"])\n",
    "\n",
    "temp[\"Percent\"] = round(temp.Total_missing/len(all_data)*100,2)\n",
    "\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c04f1c6ac4aa3bde85bb17299eebaa85d7f99ee"
   },
   "source": [
    "We are done imputing missing values. Some of the variables are in the numerical format even though they are actually categorical variables. Let's convert them from <b>int</b> to **str**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "20422a27ad03f7d6292f39ae8ac40f402bb5f452"
   },
   "outputs": [],
   "source": [
    "## the \"OverallCond\" and \"OverallQual\" of the house. \n",
    "all_data['OverallCond'] = all_data['OverallCond'].astype(str) \n",
    "all_data['OverallQual'] = all_data['OverallQual'].astype(str)\n",
    "\n",
    "## Important years and months that should be categorical variables not numerical. \n",
    "all_data['YearBuilt'] = all_data['YearBuilt'].astype(str)\n",
    "all_data['YearRemodAdd'] = all_data['YearRemodAdd'].astype(str)\n",
    "all_data['YrSold'] = all_data['YrSold'].astype(str)\n",
    "all_data['MoSold'] = all_data['MoSold'].astype(str) \n",
    "all_data['GarageYrBlt'] = all_data['GarageYrBlt'].astype(str)\n",
    "\n",
    "## Zoning class are given in numerical; therefore converted to categorical variables. \n",
    "all_data['MSSubClass'] = all_data['MSSubClass'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "46c723dc201b9261bb2ad68bf879c7414ccd0b7d"
   },
   "outputs": [],
   "source": [
    "# feture engineering a new feature \"TotalFS\"\n",
    "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "47191c746da4a7263991c56a2c13394e11e0414b"
   },
   "source": [
    "Let's see how skewed our predictor features are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "e0298795bb02bf3c8457a70b59bb3dd140467114"
   },
   "outputs": [],
   "source": [
    "## Import skew from scipy. \n",
    "from scipy.stats import skew\n",
    "## Getting all the data that are not of \"object\" type. \n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "\n",
    "# Check the skew of all numerical features\n",
    "skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "print(\"\\nSkew in numerical features: \\n\")\n",
    "\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "skewness.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "2551ecd44a426d2f652caf6796f66e756d00ff0b"
   },
   "outputs": [],
   "source": [
    "### skewness = skewness[abs(skewness) > 0.75]\n",
    "print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
    "\n",
    "from scipy.special import boxcox1p\n",
    "skewed_features = skewness.index\n",
    "lam = 0.15\n",
    "for feat in skewed_features:\n",
    "    #all_data[feat] += 1\n",
    "    all_data[feat] = boxcox1p(all_data[feat], lam)\n",
    "    \n",
    "#all_data[skewed_features] = np.log1p(all_data[skewed_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "73bfb92a7a8eaf6cb8ef3cbed5546e7f5b5fea78"
   },
   "source": [
    "### Creating Dummy Variables. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "04ad2e55829bd12c43eff092df82c0e1321ba213"
   },
   "outputs": [],
   "source": [
    "## Creating dummy variable \n",
    "all_data = pd.get_dummies(all_data, drop_first=True)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8c5922ee4b2a5f8a1a13b3cd34fedfe9a56cfe77"
   },
   "source": [
    "**Get the modified train and test set.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "7deaa02e78a076c70665ba4ddd8f8ec8752ad092"
   },
   "outputs": [],
   "source": [
    "## Once all the pre-processing is complete, we separate train and test dataset. \n",
    "train = all_data[:n_train]\n",
    "test = all_data[n_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b50dc31e77e5cd1929f1797849f02a714e144ac4"
   },
   "source": [
    "### Train_test split\n",
    "\n",
    "We have separated dependent and independent features; We have separated train and test data. So, why do we still have to split our training data? If you are curious about that, I have the answer. For this competition, when we train the machine learning algorithms, we use part of the training set usually two-thirds of the train data. Once we train our algorithm using 2/3 of the train data, we start to test our algorithms using the remaining data. If the model performs well we dump our test data in the algorithms to predict and submit the competition. The code below, basically splits the train data into 4 parts, **train_x**, **test_x**, **train_y**, **test_y**.  \n",
    "* **train_x** and **train_y** first used to train the algorithm. \n",
    "* then, **test_x** is used in that trained algorithms to predict **outcomes. **\n",
    "* Once we get the **outcomes**, we compare it with **test_y**\n",
    "\n",
    "By comparing the **outcome** of the model with **test_y**, we can determine whether our algorithms are performing well or not. Once we are confident about the result of our algorithm we may use the model to on the original test data and submit in the challenge. I have tried to show this whole process in the visualization chart below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "06c936035b416ec227977e3721fdf40bf777447d"
   },
   "outputs": [],
   "source": [
    "## Train test s\n",
    "from sklearn.model_selection import train_test_split\n",
    "## Train test split follows this distinguished code pattern and helps creating train and test set to build machine learning. \n",
    "train_x, test_x, train_y, test_y = train_test_split(train, y_train,test_size = .33, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "52afc83f027aa54fa18625b7898cfaa6632222fc"
   },
   "outputs": [],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(\"*\"*20)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "693dabb694752e7c071d52c8f053c36a130e2805"
   },
   "source": [
    "## Modeling the Data\n",
    "\n",
    "Before modeling each algorithm, I would like to discuss them for a better understanding. This way I would review what I know and at the same time help out the community. If you already know enough about Linear Regression, you may skip this part and go straight to the part where I fit the model. However, if you take your time to read this and other model description sections and let me know how I am doing, I would genuinely appreciate it. Let's get started. \n",
    "\n",
    "**Linear Regression**\n",
    "<div>\n",
    " \n",
    "We will start with one of the most basic but useful machine learning model, **Linear Regression**. However, do not let the simplicity of this model fool you, as Linear Regression is the base some of the most complex models out there. For the sake of understanding this model, we will use parts of our data. Let's say our dataset has two features only, They are **SalePrice** and **GrLivArea**. Let's take a sample of the data and graph it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "0ffd0409cd46705f5c88f93f07da8eaaded30a73"
   },
   "outputs": [],
   "source": [
    "sample_train = previous_train.sample(300)\n",
    "import seaborn as sns\n",
    "plt.subplots(figsize = (15,8))\n",
    "ax = plt.gca()\n",
    "ax.scatter(sample_train.GrLivArea.values, sample_train.SalePrice.values, color ='b');\n",
    "plt.title(\"Chart with Data Points\");\n",
    "#ax = sns.regplot(sample_train.GrLivArea.values, sample_train.SalePrice.values)\n",
    "#ax.plot((sample_train.GrLivArea.values.min(),sample_train.GrLivArea.values.max()), (sample_train.SalePrice.values.mean(),sample_train.SalePrice.values.mean()), color = 'r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a6c9e0f129fdfa70d0a5878c45f180c576518163"
   },
   "source": [
    "As we discussed before, there is a linear relationship between SalePrice and GrLivArea. We want to know/estimate/predict the sale price of a house based on the given area, How do we do that? One naive way is to find the average of all the house prices. Let's find a line with the average of all houses and place it in the scatter plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "5ffa20e5cb2a2bc5a03ad284cedd84632425f857"
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (15,8))\n",
    "ax = plt.gca()\n",
    "ax.scatter(sample_train.GrLivArea.values, sample_train.SalePrice.values, color ='b');\n",
    "#ax = sns.regplot(sample_train.GrLivArea.values, sample_train.SalePrice.values)\n",
    "ax.plot((sample_train.GrLivArea.values.min(),sample_train.GrLivArea.values.max()), (sample_train.SalePrice.values.mean(),sample_train.SalePrice.values.mean()), color = 'r');\n",
    "plt.title(\"Chart with Average Line\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2016054b0921f625042916a5920fd67ecae6b0b4"
   },
   "source": [
    "You can tell this is not the most efficient way to estimate the price of houses. The average line clearly does not represent all the datapoint and fails to grasp the linear relationship between <b>GrLivArea & SalePrice. </b> Let's find out the Mean Squared Error(more on this later) of this line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "c32dcd95b8879ead8269b8d40041c711b34d205d"
   },
   "outputs": [],
   "source": [
    "## Calculating Mean Squared Error(MSE)\n",
    "sample_train['mean_sale_price'] = sample_train.SalePrice.mean()\n",
    "sample_train['mse'] = np.square(sample_train.mean_sale_price - sample_train.SalePrice)\n",
    "sample_train.mse.mean()\n",
    "## getting mse\n",
    "print(\"Mean Squared Error(MSE) for average line is : {}\".format(sample_train.mse.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b55a601785ff4aec54c06b21ed13fcff8456aa61"
   },
   "source": [
    "We will explain more on MSE later. For now, let's just say, the closer the value of MSE is to \"0\", the better. Of course, it makes sense since we are talking about an error(mean squared error). We want to minimize this error. How can we do that? \n",
    "\n",
    "Introducing **Linear Regression**, one of the most basic and straightforward models. Many of us may have learned to show the relationship between two variable using something called \"y equals mX plus b.\" Let's refresh our memory and call upon on that equation.\n",
    "\n",
    "# $$ {y} = mX + b $$\n",
    "\n",
    "Here, \n",
    "* **m** = slope of the regression line. It represents the relationship between X and y. In another word, it gives weight as to for each x(horizontal space) how much y(vertical space) we have to cover. In machine learning, we call it **coefficient**. \n",
    "* **b** = y-intercept. \n",
    "* **x** and **y** are the data points located in x_axis and y_axis respectively. \n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "If you would like to know more about this equation, Please check out this [video](https://www.khanacademy.org/math/algebra/two-var-linear-equations/writing-slope-intercept-equations/v/graphs-using-slope-intercept-form). \n",
    "\n",
    "This slope equation gives us an exact linear relationship between X and y. This relationship is \"exact\" because we are given X and y beforehand and based on the value of X and y, we come up with the slope and y-intercept, which in turns determine the relationship between X and y. However, in real life, data is not that simple. Often the relationship is unknown to us, and even if we know the relationship, it may not always be exact. To fit an exact slope equation in an inexact relationship of data we introduce the term error. Let's see how mathematicians express this error with the slope equation. \n",
    "\n",
    "## $$ y = \\beta_0 + \\beta_1 x + \\epsilon \\\\ $$\n",
    "\n",
    "And, this is the equation for a simple linear regression.\n",
    "Here,\n",
    "* y = Dependent variable. This is what we are trying to estimate/solve/understand. \n",
    "* $\\beta_0$ = the y-intercept, it is a constant and it represents the value of y when x is 0. \n",
    "* $\\beta_1$ = Slope, Weight, Coefficient of x. This metrics is the relationship between y and x. In simple terms, it shows 1 unit increase in y changes when 1 unit increases in x. \n",
    "* $x_1$ = Independent variable ( simple linear regression ) /variables.\n",
    "* $ \\epsilon$ = error or residual. This error is the only part that's different/addition from the slope equation. This error exists because in real life we will never have a dataset where the regression line crosses exactly every single data point. There will be at least a good amount of points where the regression line will not be able to go through for the sake of model specifications and ** bias-variance tradeoff **(more on this later). This error term accounts for the difference of those points. So, simply speaking, an error is the difference between an original value and a predicted value(point in regression line). \n",
    "\n",
    "We use this function to predict the values of one dependent(target) variable based on one independent(predictor) variable. Therefore this regression is called **Simple linear regression(SLR).** If we were to write the equation regarding the sample example above it would simply look like the following equation, \n",
    "## $$ Sale Price= \\beta_0 + \\beta_1 (Area) + \\epsilon \\\\ $$\n",
    "\n",
    "This equation gives us a line that fits the data and often performs better than the average line above. But,\n",
    "* How do we know that Linear regression line is actually performing better than the average line? \n",
    "* What metrics can we use to answer that? \n",
    "* How do we know if this line is even the best line(best-fit line) for the dataset? \n",
    "* If we want to get even more clear on this we may start with answering, How do we find the $\\beta_0$(intercept) and  $\\beta_1$(coefficient) of the equation?\n",
    "\n",
    "<b>Finding $\\beta_0$(intercept) and  $\\beta_1$(coefficient):</b>\n",
    "\n",
    "We can use the following equation to find the $\\beta_0$(intercept) and  $\\beta_1$(coefficient)\n",
    "\n",
    "\n",
    "### $$ \\hat{\\beta}_1 = r_{XY} \\frac{s_Y}{x_X}$$\n",
    "### $$ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} $$\n",
    "\n",
    "Here...\n",
    "- $\\bar{y}$ : the sample mean of observed values $Y$\n",
    "- $\\bar{x}$ : the sample mean of observed values $X$\n",
    "- $s_Y$ : the sample standard deviation of observed values $Y$\n",
    "- $s_X$ : the sample standard deviation of observed values $X$\n",
    "- $r_{XY}$ : the sample Pearson correlation coefficient between observed $X$ and $Y$\n",
    "\n",
    "Now that we know how to find beta coefficients, let's calculate them in the coding section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "0dd50561f48cfcbc0c098ca201e9cd73214fd400"
   },
   "outputs": [],
   "source": [
    "## Calculating the beta coefficients by hand. \n",
    "## mean of y. \n",
    "y_bar = sample_train.SalePrice.mean()\n",
    "## mean of x. \n",
    "x_bar = sample_train.GrLivArea.mean()\n",
    "## Std of y\n",
    "std_y = sample_train.SalePrice.std()\n",
    "## std of x\n",
    "std_x = sample_train.GrLivArea.std()\n",
    "## correlation of x and y\n",
    "r_xy = sample_train.corr().loc['GrLivArea','SalePrice']\n",
    "## finding beta_1\n",
    "beta_1 = r_xy*(std_y/std_x)\n",
    "## finding beta_0\n",
    "beta_0 = y_bar - beta_1*x_bar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "40fb0b2e84937a5e857e4b8e6c4a7bb8917db3d6"
   },
   "source": [
    "So, we have calculated the beta coefficients.  We can now plug them in the linear equation to get the predicted y value. Let's do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "2b760816127f1862fbcce20bfbd37ad8f890fef0"
   },
   "outputs": [],
   "source": [
    "## getting y_hat, which is the predicted y values. \n",
    "sample_train['Linear_Yhat'] = beta_0 + beta_1*sample_train['GrLivArea']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d41e485c22b38470418498f539cb11c5b1a46683"
   },
   "source": [
    "Now that we have our predicted y values let's see how the predicted regression line looks in the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "a55a5113a0af287a76f1a7447ae280bf44cc6184"
   },
   "outputs": [],
   "source": [
    "# create a figure\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "# get the axis of that figure\n",
    "ax = plt.gca()\n",
    "\n",
    "# plot a scatter plot on it with our data\n",
    "ax.scatter(sample_train.GrLivArea, sample_train.SalePrice, c='b')\n",
    "ax.plot(sample_train['GrLivArea'], sample_train['Linear_Yhat'], color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "26bfc81390d1aa50a1d016e9c5f11196b55b14c0"
   },
   "source": [
    "Phew!! This looks like something we can work with!! Let's find out the MSE for the regression line as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "30db9e460173e8daa82c57851f2d50f303f68031"
   },
   "outputs": [],
   "source": [
    "## getting mse\n",
    "print(\"Mean Squared Error(MSE) for regression line is : {}\".format(np.square(sample_train['SalePrice'] - sample_train['Linear_Yhat']).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1f8908df4ab3a6beda3e2a647c6a067213ab2eba"
   },
   "source": [
    "A much-anticipated decrease in mean squared error(mse), therefore better-predicted model. The way we compare between the two predicted lines is by considering their errors. Let's put both of the model's side by side and compare the errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "a474754c149811fa0df7dbc5f92b29fb824a9332"
   },
   "outputs": [],
   "source": [
    "## Creating a customized chart. and giving in figsize and everything. \n",
    "fig = plt.figure(constrained_layout=True, figsize=(15,5))\n",
    "## creating a grid of 3 cols and 3 rows. \n",
    "grid = gridspec.GridSpec(ncols=2, nrows=1, figure=fig)\n",
    "#gs = fig3.add_gridspec(3, 3)\n",
    "#ax1 = fig.add_subplot(grid[row, column])\n",
    "ax1 = fig.add_subplot(grid[0, :1])\n",
    "\n",
    "# get the axis\n",
    "ax1 = fig.gca()\n",
    "\n",
    "# plot it\n",
    "ax1.scatter(x=sample_train['GrLivArea'], y=sample_train['SalePrice'], c='b')\n",
    "ax1.plot(sample_train['GrLivArea'], sample_train['mean_sale_price'], color='k');\n",
    "\n",
    "# iterate over predictions\n",
    "for _, row in sample_train.iterrows():\n",
    "    plt.plot((row['GrLivArea'], row['GrLivArea']), (row['SalePrice'], row['mean_sale_price']), 'r-')\n",
    "    \n",
    "ax2 = fig.add_subplot(grid[0, 1:])\n",
    "\n",
    "# plot it\n",
    "ax2.scatter(x=sample_train['GrLivArea'], y=sample_train['SalePrice'], c='b')\n",
    "ax2.plot(sample_train['GrLivArea'], sample_train['Linear_Yhat'], color='k');\n",
    "# iterate over predictions\n",
    "for _, row in sample_train.iterrows():\n",
    "    plt.plot((row['GrLivArea'], row['GrLivArea']), (row['SalePrice'], row['Linear_Yhat']), 'r-')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f9c9e77595e718c2f57f3f5a09c1715e767c68b9"
   },
   "source": [
    "On the two charts above, the left one is the average line, and the right one is the regression line. <font color=\"blue\"><b>Blue</b></font> dots are observed data points and <font color=\"red\"><b>red</b></font> lines are error distance from model predicted line. As you can see the regression line reduces much of the errors; therefore, performs much better than average line. \n",
    "\n",
    "Now, we need to introduce a couple of metrics that will help us compare and contrast models. One of them is mean squared error(mse) which we used while comparing two models. Some of the other metrics are...\n",
    "\n",
    "\n",
    "* Residual Sum of the Squared(RSS) or Sum of the Squared Error(SSE)\n",
    "* Coefficient of the determination ($R^2$)\n",
    "\n",
    "Let's go through each one of them. \n",
    " \n",
    " ### Residual Sum of the Squared(RSS)/Sum of the Squared Error(SSE): \n",
    "These two are the same metrics and known in different names. Similar to MSE, RSS determines how far away the observed values are from the regression line. However, the calculation is different. RSS is calculated by squaring each error/residual of the data points and then adding them together, While MSE is calculated by taking the difference between the mean of the target variable from each data points and then squaring them.   In better models, both RSS and MSE scores are closer to **\"0\"**. Let's look that the equation for finding RSS/SSE\n",
    "\n",
    "\n",
    "### $$RSS = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 $$\n",
    "\n",
    "Here, \n",
    "* We find the difference of each observed($Y_i$) value and predicted($\\hat{Y}_i$) value. Here predicted value is basically what the regression line suggests for that specific independent variable.\n",
    "* We squared the difference for each data point. \n",
    "* We add them all together. Hence, the name **Residual Sum of squares(RSS)**.\n",
    "\n",
    "A visualization would make things much more clear. \n",
    " ![](http://blog.hackerearth.com/wp-content/uploads/2016/12/anat.png)\n",
    " \n",
    "Here the blue line is the <font color=\"blue\"><b>regression line</b></font> and the black dot is the observed value. So, RSS describes the distance between the black dot and the <font color=\"blue\"><b>regression line</b></font>. The closer the regression line to the black dot(observed value), the better the model is. \n",
    "\n",
    "And, we already know ...\n",
    "## $$ \\hat{y} = \\beta_0 + \\beta_1 x + \\epsilon \\\\ $$\n",
    "\n",
    "Let's plug in( $\\hat{Y}$  ) equation in the RSS equation and we get...\n",
    "$$RSS = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 $$\n",
    "\n",
    "This equation is also known as the loss function. Here, **\"loss\"** is the sum of squared residuals(More on this later). \n",
    "\n",
    "### Mean Squared Error\n",
    "Now let's get back to our naive prediction and calculate the **Mean squared error**, which is also a metrics similar to RSS, helps us determine how well our model is performing. In **Mean squared error** we subtract the mean of y from each y datapoints and square them. \n",
    "\n",
    "### Coefficient of the Determination($R^2$)\n",
    "\n",
    "![](https://www.machinelearningplus.com/wp-content/uploads/2017/03/R_Squared_Computation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a63b3384919bafbfe8139b0d7934d6669f780ee6"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4086feea48f5c896016ece5bb69462fca1b5519c"
   },
   "source": [
    "If you would like to improve this result further, you can think about the assumptions of the linear regressions and apply them as we have discussed earlier in this kernel. \n",
    "\n",
    "\n",
    "Similar to **Simple Linear Regression**, there is an equation for multiple independent variables to predict a target variable. The equation is as follows.\n",
    "\n",
    "## $$ \\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n $$\n",
    "\n",
    "Here, We can already know parts of the equation, and from there we can keep adding new features and their coefficients with the equations. Quite simple, isn't it. \n",
    "\n",
    "We can have a target variable predicted by multiple independent variables using this equation. Therefore this equation is called **Multiple Linear Regression.** Let's try this regression in the housing dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6629786bbccd98ebafcdc9b96dd824f004980850"
   },
   "outputs": [],
   "source": [
    "## importing necessary models.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score\n",
    "\n",
    "## Call in the LinearRegression object\n",
    "lin_reg = LinearRegression(normalize=True, n_jobs=-1)\n",
    "## fit train and test data. \n",
    "lin_reg.fit(train_x, train_y)\n",
    "## Predict test data. \n",
    "y_pred = lin_reg.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f17be9689dc33f97874e324db87fd5867ea7d475"
   },
   "outputs": [],
   "source": [
    "## get average squared error(MSE) by comparing predicted values with real values. \n",
    "print ('%.2f'%mean_squared_error(test_y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9a26fa848c9a15216230fb7fbc1b8d9b87f79ccd"
   },
   "source": [
    "## Using cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "19db9a7007779c12bce754e28d1685d585e216ef"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\n",
    "lin_reg = LinearRegression()\n",
    "cv = KFold(shuffle=True, random_state=2, n_splits=10)\n",
    "scores = cross_val_score(lin_reg, train,y_train,cv = cv, scoring = 'neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "75bad1b5f175bb1e63080aa920fa6c59c3f529de"
   },
   "outputs": [],
   "source": [
    "print ('%.8f'%scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "832d9891a129f5a2e7762fd4f347844db1c70ad0"
   },
   "source": [
    " This way of model fitting above is probably the simplest way to construct a machine learning model. However, Let's dive deep into some more complex regression. \n",
    "\n",
    "### Regularization Models\n",
    "Among all the machine learning models, some of my favorites are **Regression models**. What makes regression model even more effective is its ability of *regularizing*. The term \"regularizing\" stands for models ability **to structurally prevent overfitting by imposing a penalty on the coefficients.** \n",
    "\n",
    "\n",
    "There are three types of regularizations. \n",
    "* **Ridge**\n",
    "* **Lasso**\n",
    "* **Elastic Net**\n",
    "\n",
    "These regularization methods work by penalizing **the magnitude of the coefficients of features** and at the same time **minimizing the error between the predicted value and actual observed values**.  This minimization becomes a balance between the error (the difference between the predicted value and observed value) and the size of the coefficients. The only difference between Ridge and Lasso is **the way they penalize the coefficients.** Elastic Net is the combination of these two. **Elastic Net** adds both the sum of the squares errors and the absolute value of the squared error. To get more in-depth of it, let us review the least squared loss function. \n",
    "\n",
    "**Ordinary least squared** loss function minimizes the residual sum of the square(RSS) to fit the data:\n",
    "\n",
    "### $$ \\text{minimize:}\\; RSS = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 $$\n",
    "\n",
    "Let's review this equation once again, Here: \n",
    "* $y_i$ is the observed value. \n",
    "* $\\hat{y}_i$ is the predicted value. \n",
    "* The error = $y_i$ - $\\hat{y}_i$\n",
    "* The square of the error = $(y_i - \\hat{y}_i)^2$\n",
    "* the sum of the square of the error = $\\sum_{i=1}^n (y_i - \\hat{y}_i)^2$, that's the equation on the left. \n",
    "* the only difference between left sides equation vs. the right sides one above is the replacement of $\\hat{y}_i$, it is replaced by $\\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)$, which simply follow's the slope equation, y = mx+b, where, \n",
    " * $\\beta_0$ is the intercept. \n",
    " * **$\\beta_j$ is the coefficient of the feature($x_j$).**\n",
    " \n",
    "Let's describe the effect of regularization and then we will learn how we can use loss function in Ridge.\n",
    "* One of the benefits of regularization is that it deals with **multicollinearity** well, especially Ridge method. Lasso deals with **multicollinearity** more brutally by penalizing related coefficients and force them to become zero, hence removing them. However, **Lasso** is well suited for redundant variables. \n",
    " \n",
    "***\n",
    "<div>\n",
    "    \n",
    " #### Ridge:\n",
    "Ridge regression adds penalty equivalent to the square of the magnitude of the coefficients. This penalty is added to the least square loss function above and looks like this...\n",
    "\n",
    "### $$ \\text{minimize:}\\; RSS+Ridge = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\lambda_2\\sum_{j=1}^p \\beta_j^2$$\n",
    "\n",
    "Here, \n",
    "* $\\lambda_2$ is constant; a regularization parameter. It is also known as $\\alpha$. The higher the value of this constant the more the impact in the loss function. \n",
    "    * When $\\lambda_2$ is 0, the loss funciton becomes same as simple linear regression. \n",
    "    * When $\\lambda_2$ is $\\infty$, the coefficients become 0\n",
    "    * When $\\lambda_2$ is between  0 and $\\infty$(0<$\\lambda_2$<$\\infty$), The $\\lambda_2$ parameter will decide the miagnitude given to the coefficients. The coefficients will be somewhere between 0 and ones for simple linear regression. \n",
    "* $\\sum_{j=1}^p \\beta_j^2$, is the squared sum of all coefficients. \n",
    "\n",
    "Now that we know every nitty-gritty details about this equation, let's use it for science, but before that a couple of things to remember. \n",
    "* It is essential to standardize the predictor variables before constructing the models. \n",
    "* It is important to check for multicollinearity, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "05331dd97c776130ce8e3d2ade9028847c4c3c7d"
   },
   "outputs": [],
   "source": [
    "## Importing Ridge. \n",
    "from sklearn.linear_model import Ridge\n",
    "## Assiging different sets of alpha values to explore which can be the best fit for the model. \n",
    "alpha_ridge = [-3,-2,-1,1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1,1.5, 2,3,4, 5, 10, 20, 30, 40]\n",
    "temp_rss = {}\n",
    "temp_mse = {}\n",
    "for i in alpha_ridge:\n",
    "    ## Assigin each model. \n",
    "    ridge = Ridge(alpha= i, normalize=True)\n",
    "    ## fit the model. \n",
    "    ridge.fit(train_x, train_y)\n",
    "    ## Predicting the target value based on \"Test_x\"\n",
    "    y_pred = ridge.predict(test_x)\n",
    "\n",
    "    mse = mean_squared_error(test_y, y_pred)\n",
    "    rss = sum((y_pred-test_y)**2)\n",
    "    temp_mse[i] = mse\n",
    "    temp_rss[i] = rss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "04989382e217c64e7e0b9c198512ac38c2b8a9c9"
   },
   "outputs": [],
   "source": [
    "temp_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8c84d3116dce02f5cb4191b5bb3e5a638d9ce804"
   },
   "outputs": [],
   "source": [
    "temp_rss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f4cc00346f4cc9fa98ba4dc07f4c0377bf3bc334"
   },
   "source": [
    "#### Lasso:\n",
    "Lasso adds penalty equivalent to the absolute value of the sum of coefficients. This penalty is added to the least square loss function and replaces the squared sum of coefficients from Ridge. \n",
    "\n",
    "### $$ \\text{minimize:}\\; RSS + Lasso = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\lambda_1\\sum_{j=1}^p |\\beta_j|$$\n",
    "\n",
    "Here, \n",
    "* $\\lambda_2$ is a constant similar to the Ridge function. \n",
    "* $\\sum_{j=1}^p |\\beta_j|$ is the absolute sum of the coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1e3bd95258d3f5b59b4278b20ebb41b722767b43"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "temp_rss = {}\n",
    "temp_mse = {}\n",
    "for i in alpha_ridge:\n",
    "    ## Assigin each model. \n",
    "    lasso_reg = Lasso(alpha= i, normalize=True)\n",
    "    ## fit the model. \n",
    "    lasso_reg.fit(train_x, train_y)\n",
    "    ## Predicting the target value based on \"Test_x\"\n",
    "    y_pred = lasso_reg.predict(test_x)\n",
    "\n",
    "    mse = mean_squared_error(test_y, y_pred)\n",
    "    rss = sum((y_pred-test_y)**2)\n",
    "    temp_mse[i] = mse\n",
    "    temp_rss[i] = rss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ec39ca65873af60c22629903fec39cdb0c3d78bf"
   },
   "outputs": [],
   "source": [
    "temp_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "58b0406c520895d4bbcaa5e31288be9cd13b1f9f"
   },
   "source": [
    "#### Elastic Net: \n",
    "Elastic Net is the combination of both Ridge and Lasso. It adds both the sum of squared coefficients and the absolute sum of the coefficients with the ordinary least square function. Let's look at the function. \n",
    "\n",
    "### $$ \\text{minimize:}\\; RSS + Ridge + Lasso = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\lambda_1\\sum_{j=1}^p |\\beta_j| + \\lambda_2\\sum_{j=1}^p \\beta_j^2$$\n",
    "\n",
    "I guess this equation is pretty self-explanatory if you have been following this kernel so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "64079c0c5f0dabb6b9349a4836839b2af0e29dd7"
   },
   "source": [
    "## Credits. \n",
    "* To GA where I started my data science journey.\n",
    "* To Kaggle community for inspiring me over and over again. \n",
    "* Credits to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7798e06ca6dad38596069fb51bf4eb57bef4f192"
   },
   "source": [
    "***\n",
    "If you like to discuss any other projects or just have a chat about data science topics, I'll be more than happy to connect with you on:\n",
    "\n",
    "<b><a href=\"https://www.linkedin.com/in/masumrumi/\">LinkedIn</a></b>\n",
    "\n",
    "\n",
    "**LinkedIn:** https://www.linkedin.com/in/masumrumi/ \n",
    "\n",
    "**My Website:** http://masumrumi.strikingly.com/ \n",
    "\n",
    "*** This kernel will always be a work in progress. I will incorporate new concepts of data science as I comprehend them with each update. If you have any idea/suggestions about this notebook, please let me know. Any feedback about further improvements would be genuinely appreciated.***\n",
    "***\n",
    "### If you have come this far, Congratulations!!\n",
    "\n",
    "### If this notebook helped you in any way or you liked it, please upvote and/or leave a comment!! :) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8c84dfc5566dcb751d8ddb9178c941eb42963022"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4f28fcef967ecb0188435ee0dea41d73505c7c99"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d71a607e47da976ed0c76ef8a9dd10cddc62d5a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0c3ce066db03c0108c691ee33a226ee9ed256df5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f661bd49239446605eff8e86ae4c69e7bd79f1c1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
